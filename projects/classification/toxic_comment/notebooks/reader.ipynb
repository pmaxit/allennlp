{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Iterable\n",
    "import csv\n",
    "import sys\n",
    "import re\n",
    "\n",
    "import tqdm\n",
    "from allennlp.common import Params\n",
    "from allennlp.common.checks import ConfigurationError\n",
    "from allennlp.data import DatasetReader, Instance\n",
    "from allennlp.data.fields import TextField, LabelField, ListField\n",
    "from allennlp.data.tokenizers import Tokenizer, SpacyTokenizer, WhitespaceTokenizer\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from nltk.corpus import stopwords\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.data.batch import Batch\n",
    "from allennlp.common.util import ensure_list\n",
    "from overrides import overrides\n",
    "import numpy as np\n",
    "import torch\n",
    "from allennlp.data.fields import TextField, ArrayField\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicReader(DatasetReader):\n",
    "    \"\"\" Toxic Dataset Reader\"\"\"\n",
    "    def __init__(self, max_length:int = None, tokenizer: Tokenizer=None,\n",
    "            token_indexers: Dict[str, TokenIndexer] = None,\n",
    "            fill_in_empty_labels: bool = False,\n",
    "            clean_text:bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self._max_sequence_length = max_length\n",
    "        self.fill_in_empty_labels = fill_in_empty_labels\n",
    "        self._tokenizer = tokenizer or WhitespaceTokenizer()\n",
    "        self._token_indexer = token_indexers or {'tokens': SingleIdTokenIndexer()}\n",
    "        self._clean_text = clean_text\n",
    "\n",
    "    @overrides\n",
    "    def _read(self, file_path: str, skip_header:bool=True)->Iterable[Instance]:\n",
    "        with open(file_path, 'r') as data_file:\n",
    "            reader = csv.reader(data_file, quotechar='\"', delimiter =',', quoting=csv.QUOTE_ALL, skipinitialspace=True)\n",
    "            if skip_header:\n",
    "                next(reader)\n",
    "\n",
    "            for row in reader:\n",
    "                _, text, *labels = row\n",
    "                yield self.text_to_instance(text, labels)\n",
    "\n",
    "    @overrides\n",
    "    def text_to_instance(self,\n",
    "                text: str,\n",
    "                labels: List[str] = None)->Instance:\n",
    "            # first clean text\n",
    "            if self._clean_text:\n",
    "                text = clean_text(text)\n",
    "            \n",
    "            if self._max_sequence_length is not None:\n",
    "                text = text[:self._max_sequence_length]\n",
    "\n",
    "            tokenized_text = self._tokenizer.tokenize(text)\n",
    "            text_field = TextField(tokenized_text, self._token_indexer)\n",
    "            fields = {'text': text_field}\n",
    "\n",
    "            if labels or self.fill_in_empty_labels:\n",
    "                labels = labels or [0, 0, 0, 0, 0, 0]\n",
    "\n",
    "                toxic ,severe_toxic, obscene, threat, insult, identity_hate = labels\n",
    "                fields['labels'] = ListField([\n",
    "                    LabelField(int(toxic), skip_indexing=True),\n",
    "                    LabelField(int(severe_toxic), skip_indexing=True),\n",
    "                    LabelField(int(obscene), skip_indexing=True),\n",
    "                    LabelField(int(threat), skip_indexing=True),\n",
    "                    LabelField(int(insult), skip_indexing=True),\n",
    "                    LabelField(int(identity_hate), skip_indexing=True)\n",
    "                ])  \n",
    "\n",
    "            return Instance(fields)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = ToxicReader(max_length = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = ensure_list(reader.read('../data/train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Explanation', 'Why', 'the', 'edits', 'made', 'under', 'my', 'username', 'Hardcore', 'Metallica', 'Fan', 'were', 'reverted?', 'They', \"weren't\", 'vandalisms,', 'just', 'closure', 'on', 'some', 'GAs', 'after', 'I', 'voted', 'at', 'New', 'York', 'Dolls', 'FAC.', 'And', 'please', \"don't\", 'remove', 'the', 'template', 'from', 'the', 'talk', 'page', 'since', \"I'm\", 'retired', 'now.89.205.38.27'] [0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# print human readable form \n",
    "print(instances[0].fields['text'].human_readable_repr(),\n",
    "instances[0].fields['labels'].human_readable_repr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Till this point, fields are not indexed. Indexers have not been run because it needs vocab to convert them into numbers / indices. Let's give it a vocab now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.vocabulary import Vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130a5395a5624e289a42e419e5e9b3c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "building vocab:   0%|          | 0/159571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "vocab = Vocabulary.from_instances(instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances[0].index_fields(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': {'tokens': {'tokens': tensor([ 20619,    255,      2,    158,    119,    180,     30,    808,  14913,\n",
       "            25808,  10615,     79,  16568,    315,   2356,  72513,     50,  10494,\n",
       "               14,     62,  17355,    154,      7,   3242,     33,    421,   1500,\n",
       "            33175,  12040,    123,    102,     63,    224,      2,    539,     29,\n",
       "                2,     65,     41,    167,     72,   5350, 184237])}},\n",
       " 'labels': tensor([0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances[0].as_tensor_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': {'tokens___tokens': 43}, 'labels': {'num_fields': 6}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances[0].get_padding_lengths()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using multiple Indexers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.token_indexers import TokenCharactersIndexer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "token_character_indexer = TokenCharactersIndexer(min_padding_length=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = ToxicReader(max_length = 5000, token_indexers={'tokens': SingleIdTokenIndexer(), 'token_characters': token_character_indexer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = ensure_list(reader.read('../data/train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0487fa2d56af473b95fec53c6cfd5a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "building vocab:   0%|          | 0/159571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "vocab = Vocabulary.from_instances(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances[0].index_fields(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': {'tokens': {'tokens': tensor([ 20619,    255,      2,    158,    119,    180,     30,    808,  14913,\n",
       "            25808,  10615,     79,  16568,    315,   2356,  72513,     50,  10494,\n",
       "               14,     62,  17355,    154,      7,   3242,     33,    421,   1500,\n",
       "            33175,  12040,    123,    102,     63,    224,      2,    539,     29,\n",
       "                2,     65,     41,    167,     72,   5350, 184237])},\n",
       "  'token_characters': {'token_characters': tensor([[34, 47, 17, 11,  4,  7,  4,  3,  6,  5,  7,  0,  0,  0,  0,  0],\n",
       "           [32, 10, 16,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [ 3, 10,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [ 2, 12,  6,  3,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [15,  4, 12,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [13,  7, 12,  2,  9,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [15, 16,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [13,  8,  2,  9,  7,  4, 15,  2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [38,  4,  9, 12, 14,  5,  9,  2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [45,  2,  3,  4, 11, 11,  6, 14,  4,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [53,  4,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [20,  2,  9,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [ 9,  2, 24,  2,  9,  3,  2, 12, 55,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [28, 10,  2, 16,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [20,  2,  9,  2,  7, 29,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [24,  4,  7, 12,  4, 11,  6,  8, 15,  8, 25,  0,  0,  0,  0,  0],\n",
       "           [56, 13,  8,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [14, 11,  5,  8, 13,  9,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [ 5,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [ 8,  5, 15,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [57, 30,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [ 4, 19,  3,  2,  9,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [26,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [24,  5,  3,  2, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [ 4,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [37,  2, 20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [58,  5,  9, 23,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [49,  5, 11, 11,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [53, 30, 33, 22,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [30,  7, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [17, 11,  2,  4,  8,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [12,  5,  7, 29,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [ 9,  2, 15,  5, 24,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [ 3, 10,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [ 3,  2, 15, 17, 11,  4,  3,  2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [19,  9,  5, 15,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [ 3, 10,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [ 3,  4, 11, 23,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [17,  4, 18,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [ 8,  6,  7, 14,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [26, 29, 15,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [ 9,  2,  3,  6,  9,  2, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [ 7,  5, 20, 22, 69, 63, 22, 51, 42, 66, 22, 64, 69, 22, 51, 71]])}},\n",
       " 'labels': tensor([0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances[0].as_tensor_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': {'tokens___tokens': 43,\n",
       "  'token_characters___token_characters': 43,\n",
       "  'token_characters___num_token_characters': 16},\n",
       " 'labels': {'num_fields': 6}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances[0].get_padding_lengths()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "baf9b38691353db4aaced5eebb5c5d067bde62f8d297a23724848dd35f93e574"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
